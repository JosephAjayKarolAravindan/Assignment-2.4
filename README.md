1.	Hadoop in layman’s terms:

Hadoop is an open source, Java-based programming framework that supports the processing and storage of extremely large data sets in a distributed computing environment.

Hadoop makes it possible to run applications on systems with thousands of commodity hardware nodes, and to handle thousands of terabytes of data. Its distributed file system facilitates rapid data transfer rates among nodes and allows the system to continue operating in case of a node failure. This approach lowers the risk of catastrophic system failure and unexpected data loss, even if a significant number of nodes become inoperative. Consequently, Hadoop quickly emerged as a foundation for big data processing tasks, such as scientific analytics, business and sales planning, and processing enormous volumes of sensor data, including from internet of things sensors.

2.	Explain the components of Hadoop framework:

The 3 core components of the Apache Software Foundation’s Hadoop framework are:
•	MapReduce – A software programming model for processing large sets of data in parallel
•	HDFS – The Java-based distributed file system that can store all kinds of data without prior organization.
•	YARN – A resource management framework for scheduling and handling resource requests from distributed applications.




	

3.	Explain the reasons to learn Big data technologies:

The reasons to learn Big Data Technologies are 

•	Data analytics is now a priority for top organizations
•	Increasing job opportunities
•	Increasing pay for data analytics professionals
•	Big data analytics is everywhere
•	core of decision-making in the company
•	Adoption rate of big data analytics is high
•	Data analytics is taking over faster than expected
•	It represents perfect freelancing opportunities
•	Develop new revenue streams
